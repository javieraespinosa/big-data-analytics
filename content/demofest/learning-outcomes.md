---
widget: blank
headless: true
weight: 40

title: Learning outcomes
---

Regarding Spark you will be able to produce code to:

-   Prepare a Spark environment in an externalised computing environment and be able to schematically explain the underlying functional architecture.

-   Trigger a Spark environment from a Python notebook (understand the role and architecture promoted by the PySpark library).

-   Upload an unstructured data collection into your environment and transform it into a tabular representation (i.e. make sure that you can explain the necessity of this transformation and its benefits related to the use of Spark)

-   Design an exploration pipeline using Spark operators for selecting, projecting, correlating (and joining), grouping, and aggregating tabular data.

-   Explain the purpose of an exploration pipeline and enumerate the type of knowledge produced by this kind of task.

Regarding visualization tools necessary to understand and disseminate ([data storytelling](https://www.forbes.com/sites/brentdykes/2016/03/31/data-storytelling-the-essential-data-science-skill-everyone-needs/)) exploration results, you will be able to:

-   Import visualization tools into your notebook

-   Produce code to feed visualization functions

-   Provide insight (textual) to describe input data and interpret data processing results

-   Understand the architecture behind your notebook with the storage services, Spark data processing and the visualization tool(s).
